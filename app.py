import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from src.model_loader import load_models_for_target, get_available_targets
from src.preprocessor import load_and_preprocess_data, prepare_features_for_model

# ----------------------------
# Configuration Streamlit
# ----------------------------
st.set_page_config(
    page_title="ML Model Deployment - Global Superstore",
    page_icon="ğŸª",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalisÃ©
st.markdown("""
    <style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .prediction-box {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        text-align: center;
        font-size: 2rem;
        font-weight: bold;
        margin: 2rem 0;
        box-shadow: 0 10px 30px rgba(0,0,0,0.2);
    }
    .stButton>button {
        width: 100%;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        font-weight: bold;
        padding: 0.75rem;
        border-radius: 10px;
        border: none;
        font-size: 1.1rem;
    }
    </style>
""", unsafe_allow_html=True)

# ----------------------------
# Titre principal
# ----------------------------
st.markdown('<h1 class="main-header">ğŸª Global Superstore - ML Model Deployment</h1>', unsafe_allow_html=True)
st.markdown("### ğŸ“Š DÃ©ploiement et Comparaison de ModÃ¨les de RÃ©gression")
st.markdown("---")

# ----------------------------
# Sidebar - Configuration
# ----------------------------
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    with st.expander("â„¹ï¸ Ã€ propos"):
        st.write("""
        **Projet:** PrÃ©diction avec Global Superstore
        
        **ModÃ¨les disponibles:**
        - Linear Regression
        - Random Forest Champion
        - LightGBM variants
        - Voting Regressor
        
        **Dataset:** Global_Superstore_FIXED.csv (prÃ©traitÃ©)
        """)
    
    st.markdown("---")
    
    app_mode = st.radio(
        "Mode d'utilisation",
        ["ğŸ”® PrÃ©diction Simple", "ğŸ“Š Comparaison de ModÃ¨les", "ğŸ“ˆ Analyse du Dataset"]
    )
    
    st.markdown("---")
    
    with st.expander("âš™ï¸ ParamÃ¨tres avancÃ©s"):
        show_model_details = st.checkbox("Afficher les dÃ©tails du modÃ¨le", value=False)
        show_input_summary = st.checkbox("Afficher le rÃ©sumÃ© des inputs", value=True)

# ----------------------------
# Chargement du dataset AVEC PRÃ‰TRAITEMENT
# ----------------------------
@st.cache_data
def load_data():
    """Charger et prÃ©traiter le dataset"""
    try:
        df = load_and_preprocess_data("data/Global_Superstore_FIXED.csv")
        st.success(f"âœ… DonnÃ©es prÃ©traitÃ©es: {df.shape[0]:,} lignes Ã— {df.shape[1]} colonnes")
        return df
    except FileNotFoundError:
        st.error("âŒ Fichier Global_Superstore_FIXED.csv non trouvÃ©!")
        return None
    except Exception as e:
        st.error(f"âŒ Erreur: {str(e)}")
        return None

df = load_data()

if df is None:
    st.stop()

# Colonnes numÃ©riques
numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()

if len(numeric_columns) == 0:
    st.error("âŒ Aucune colonne numÃ©rique!")
    st.stop()

# Targets disponibles
available_targets = get_available_targets()

# ----------------------------
# MODE 1: PRÃ‰DICTION SIMPLE
# ----------------------------
if app_mode == "ğŸ”® PrÃ©diction Simple":
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("ğŸ¯ Configuration")
        
        # Selection du target avec info
        st.info(f"""
        **Targets disponibles avec modÃ¨les entraÃ®nÃ©s:**
        - {', '.join([f'**{t}**' for t in available_targets])}
        """)
        
        target_col = st.selectbox(
            "Variable cible Ã  prÃ©dire",
            available_targets,
            help="SÃ©lectionnez le target pour lequel charger les modÃ¨les"
        )
        
        # Charger les modÃ¨les pour ce target
        @st.cache_resource
        def get_models_for_target(target):
            """Charger les modÃ¨les avec cache"""
            try:
                return load_models_for_target(target)
            except Exception as e:
                st.error(f"âŒ Erreur: {str(e)}")
                return {}
        
        models = get_models_for_target(target_col)
        
        if not models:
            st.error(f"âŒ Aucun modÃ¨le disponible pour {target_col}!")
            st.info("ğŸ’¡ ExÃ©cutez `python train_multitarget_models.py` pour entraÃ®ner les modÃ¨les")
            st.stop()
        
        model_name = st.selectbox(
            "ModÃ¨le Ã  utiliser",
            list(models.keys()),
            help="Choisissez le modÃ¨le"
        )
        
        model = models[model_name]
        
        if show_model_details:
            st.info(f"""
            **ModÃ¨le:** {model_name}
            **Type:** {type(model).__name__}
            **Target:** {target_col}
            """)
            
            if hasattr(model, 'feature_names_in_'):
                st.info(f"**Features attendues:** {len(model.feature_names_in_)}")
    
    with col2:
        st.subheader("ğŸ“Š AperÃ§u des DonnÃ©es")
        st.dataframe(df.head(10), width="stretch")
        
        col_stats1, col_stats2, col_stats3 = st.columns(3)
        with col_stats1:
            st.metric("ğŸ“ Lignes", f"{df.shape[0]:,}")
        with col_stats2:
            st.metric("ğŸ“Š Colonnes", df.shape[1])
        with col_stats3:
            st.metric("ğŸ”¢ NumÃ©riques", len(numeric_columns))
    
    st.markdown("---")
    
    # PrÃ©parer les features
    X, y, missing_features = prepare_features_for_model(df, target_col, model)
    
    if missing_features:
        # VÃ©rifier si le problÃ¨me est que la target est incompatible
        if target_col in missing_features:
            # Le modÃ¨le attend cette colonne comme feature, pas comme target
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if hasattr(model, 'feature_names_in_'):
                expected_features = list(model.feature_names_in_)
                possible_targets = [col for col in numeric_cols if col not in expected_features and col != target_col]
            else:
                possible_targets = []
            
            st.error(f"âŒ Ce modÃ¨le ne peut PAS prÃ©dire '{target_col}'")
            st.warning(f"""
            ğŸ” **Raison**: Le modÃ¨le '{model_name}' attend '{target_col}' comme **entrÃ©e** (feature), 
            pas comme **sortie** (target Ã  prÃ©dire).
            """)
            
            if possible_targets:
                st.info(f"""
                ğŸ’¡ **Solution**: Ce modÃ¨le a Ã©tÃ© entraÃ®nÃ© pour prÃ©dire l'une de ces variables:
                
                {', '.join([f'**{t}**' for t in possible_targets[:5]])}
                
                â¡ï¸ Changez la "Variable cible Ã  prÃ©dire" vers l'une de ces options.
                """)
            else:
                st.info("ğŸ’¡ Essayez un autre modÃ¨le ou rÃ©entraÃ®nez les modÃ¨les avec la bonne cible.")
        else:
            # Autres features manquantes
            st.error(f"âŒ Ce modÃ¨le nÃ©cessite {len(missing_features)} features manquantes")
            st.info("ğŸ’¡ Essayez 'Linear Regression' ou 'Random Forest Champion'")
        
        st.stop()
    
    st.success(f"""
    âœ… **Configuration validÃ©e:**
    - Variable cible: **{target_col}**
    - Features: **{X.shape[1]}** colonnes
    - ModÃ¨le: **{model_name}**
    """)
    
    # Inputs utilisateur
    st.subheader("ğŸ”§ ParamÃ¨tres de PrÃ©diction")
    
    num_cols = 3
    cols = st.columns(num_cols)
    
    input_data = {}
    
    for idx, col in enumerate(X.columns):
        with cols[idx % num_cols]:
            col_min = float(X[col].min())
            col_max = float(X[col].max())
            col_mean = float(X[col].mean())
            
            # Pour les colonnes binaires, utiliser checkbox
            if X[col].nunique() == 2 and set(X[col].unique()).issubset({0, 1, 0.0, 1.0}):
                input_data[col] = float(st.checkbox(
                    label=f"{col}",
                    value=bool(col_mean > 0.5)
                ))
            else:
                input_data[col] = st.number_input(
                    label=f"{col}",
                    min_value=col_min,
                    max_value=col_max,
                    value=col_mean,
                    help=f"Min: {col_min:.2f}, Max: {col_max:.2f}"
                )
    
    input_df = pd.DataFrame([input_data])
    
    if show_input_summary:
        with st.expander("ğŸ“‹ RÃ©sumÃ© des valeurs"):
            st.dataframe(input_df.T, width="stretch")
    
    st.markdown("---")
    
    # PrÃ©diction
    col_pred1, col_pred2, col_pred3 = st.columns([1, 2, 1])
    
    with col_pred2:
        if st.button("ğŸš€ LANCER LA PRÃ‰DICTION", key="predict_btn"):
            try:
                with st.spinner("ğŸ”„ Calcul en cours..."):
                    prediction = model.predict(input_df)
                    
                st.markdown(f"""
                <div class="prediction-box">
                    ğŸ¯ PrÃ©diction de {target_col}: {prediction[0]:.2f}
                </div>
                """, unsafe_allow_html=True)
                
                st.balloons()
                
                st.markdown("### ğŸ“Š Contexte de la PrÃ©diction")
                
                col1, col2, col3, col4 = st.columns(4)
                
                actual_mean = df[target_col].mean()
                actual_min = df[target_col].min()
                actual_max = df[target_col].max()
                
                with col1:
                    st.metric("Moyenne Dataset", f"{actual_mean:.2f}")
                with col2:
                    diff_mean = prediction[0] - actual_mean
                    st.metric("Diff. vs Moyenne", f"{diff_mean:.2f}", delta=f"{diff_mean:.2f}")
                with col3:
                    st.metric("Min Dataset", f"{actual_min:.2f}")
                with col4:
                    st.metric("Max Dataset", f"{actual_max:.2f}")
                
                # Graphique
                fig = go.Figure()
                
                fig.add_trace(go.Box(
                    y=df[target_col],
                    name='Distribution Dataset',
                    marker_color='lightblue'
                ))
                
                fig.add_trace(go.Scatter(
                    x=[0],
                    y=[prediction[0]],
                    mode='markers',
                    name='Votre PrÃ©diction',
                    marker=dict(size=20, color='red', symbol='star')
                ))
                
                fig.update_layout(
                    title=f"Position de votre prÃ©diction",
                    yaxis_title=target_col,
                    height=400
                )
                
                st.plotly_chart(fig, width="stretch")
                
            except Exception as e:
                st.error(f"âŒ Erreur: {str(e)}")

# ----------------------------
# MODE 2: COMPARAISON
# ----------------------------
elif app_mode == "ğŸ“Š Comparaison de ModÃ¨les":
    
    st.subheader("ğŸ“Š Comparaison des ModÃ¨les")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("### âš™ï¸ Configuration")
        
        st.info(f"""
        **Targets disponibles:**
        - {', '.join([f'**{t}**' for t in available_targets])}
        """)
        
        target_col = st.selectbox("Variable cible", available_targets)
        
        # Charger les modÃ¨les pour ce target
        models = load_models_for_target(target_col)
        
        if not models:
            st.error(f"âŒ Aucun modÃ¨le disponible pour {target_col}!")
            st.info("ğŸ’¡ ExÃ©cutez `python train_multitarget_models.py`")
            st.stop()
        
        selected_models = st.multiselect(
            "ModÃ¨les Ã  comparer",
            list(models.keys()),
            default=list(models.keys())
        )
        
        n_samples = st.slider(
            "Ã‰chantillons Ã  tester",
            min_value=10,
            max_value=min(1000, len(df)),
            value=100,
            step=10
        )
    
    with col2:
        st.markdown("### ğŸ“‹ ModÃ¨les Disponibles")
        
        models_info = []
        for model_name in models.keys():
            models_info.append({
                "ModÃ¨le": model_name,
                "Type": type(models[model_name]).__name__,
                "Target": target_col,
                "Statut": "âœ… PrÃªt"
            })
        
        st.dataframe(pd.DataFrame(models_info), width="stretch")
    
    st.markdown("---")
    
    if st.button("ğŸš€ COMPARER LES MODÃˆLES"):
        
        if not selected_models:
            st.warning("âš ï¸ SÃ©lectionnez au moins un modÃ¨le")
        else:
            sample_indices = np.random.choice(len(df), size=min(n_samples, len(df)), replace=False)
            df_sample = df.iloc[sample_indices]
            
            results = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, model_name in enumerate(selected_models):
                status_text.text(f"â³ Test de {model_name}...")
                
                try:
                    model = models[model_name]
                    
                    X_test, y_test, missing = prepare_features_for_model(df_sample, target_col, model)
                    
                    if missing:
                        st.warning(f"âš ï¸ {model_name}: {len(missing)} features manquantes - ignorÃ©")
                        continue
                    
                    # CRITICAL FIX: Fill NaN values before prediction (match training preprocessing)
                    if X_test.isnull().sum().sum() > 0:
                        X_test = X_test.fillna(X_test.median())
                    
                    if y_test.isnull().sum() > 0:
                        valid_idx = y_test.notna()
                        X_test = X_test[valid_idx]
                        y_test = y_test[valid_idx]
                    
                    y_pred = model.predict(X_test)
                    
                    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
                    
                    mse = mean_squared_error(y_test, y_pred)
                    rmse = np.sqrt(mse)
                    mae = mean_absolute_error(y_test, y_pred)
                    r2 = r2_score(y_test, y_pred)
                    
                    results.append({
                        "ModÃ¨le": model_name,
                        "RÂ² Score": r2,
                        "RMSE": rmse,
                        "MAE": mae,
                        "MSE": mse
                    })
                    
                except Exception as e:
                    st.error(f"âŒ {model_name}: {str(e)[:100]}")
                
                progress_bar.progress((idx + 1) / len(selected_models))
            
            status_text.text("âœ… Comparaison terminÃ©e!")
            
            if results:
                results_df = pd.DataFrame(results)
                results_df = results_df.sort_values('RÂ² Score', ascending=False)
                
                st.markdown("---")
                st.subheader("ğŸ“Š RÃ©sultats")
                
                best_model = results_df.iloc[0]['ModÃ¨le']
                best_r2 = results_df.iloc[0]['RÂ² Score']
                
                st.markdown(f"""
                <div class="prediction-box">
                    ğŸ† Meilleur: {best_model}<br>
                    RÂ² = {best_r2:.4f}
                </div>
                """, unsafe_allow_html=True)
                
                st.dataframe(
                    results_df.style.highlight_max(axis=0, subset=['RÂ² Score'], color='lightgreen')
                                   .highlight_min(axis=0, subset=['RMSE', 'MAE', 'MSE'], color='lightgreen'),
                    width="stretch"
                )
                
                col1, col2 = st.columns(2)
                
                with col1:
                    fig1 = go.Figure(data=[
                        go.Bar(
                            x=results_df['ModÃ¨le'],
                            y=results_df['RÂ² Score'],
                            text=results_df['RÂ² Score'].round(4),
                            textposition='auto',
                            marker_color='lightblue'
                        )
                    ])
                    fig1.update_layout(title="RÂ² Score", height=400)
                    st.plotly_chart(fig1, width="stretch")
                
                with col2:
                    fig2 = go.Figure(data=[
                        go.Bar(
                            x=results_df['ModÃ¨le'],
                            y=results_df['RMSE'],
                            text=results_df['RMSE'].round(2),
                            textposition='auto',
                            marker_color='lightcoral'
                        )
                    ])
                    fig2.update_layout(title="RMSE", height=400)
                    st.plotly_chart(fig2, width="stretch")
                
                csv = results_df.to_csv(index=False)
                st.download_button(
                    "ğŸ“¥ TÃ©lÃ©charger CSV",
                    csv,
                    "comparaison_modeles.csv",
                    "text/csv"
                )
            else:
                st.error("âŒ Aucun modÃ¨le n'a pu Ãªtre testÃ©")

# ----------------------------
# MODE 3: ANALYSE
# ----------------------------
elif app_mode == "ğŸ“ˆ Analyse du Dataset":
    
    st.subheader("ğŸ“ˆ Analyse Exploratoire")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ“ Lignes", f"{df.shape[0]:,}")
    with col2:
        st.metric("ğŸ“Š Colonnes", df.shape[1])
    with col3:
        st.metric("ğŸ”¢ NumÃ©riques", len(numeric_columns))
    with col4:
        st.metric("â“ Manquantes", df.isnull().sum().sum())
    
    st.markdown("---")
    
    st.subheader("ğŸ” AperÃ§u")
    
    n_rows = st.slider("Nombre de lignes", 5, 100, 10)
    st.dataframe(df.head(n_rows), width="stretch")
    
    st.markdown("---")
    
    st.subheader("ğŸ“Š Statistiques")
    st.dataframe(df[numeric_columns].describe(), width="stretch")
    
    st.markdown("---")
    
    st.subheader("ğŸ“ˆ Distribution")
    
    selected_var = st.selectbox("Variable Ã  analyser", numeric_columns)
    
    col1, col2 = st.columns(2)
    
    with col1:
        fig_hist = px.histogram(df, x=selected_var, nbins=50)
        st.plotly_chart(fig_hist, width="stretch")
    
    with col2:
        fig_box = px.box(df, y=selected_var)
        st.plotly_chart(fig_box, width="stretch")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; padding: 2rem;'>
    ğŸª Global Superstore ML Deployment<br>
    <small>Projet ML avec Streamlit</small>
</div>
""", unsafe_allow_html=True)